{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIYd7QbzdcPA",
        "outputId": "75eee346-5010-4f7c-b6dc-6ff1c4a26913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/HW4&Final/FINAL\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "path=\"/content/drive/MyDrive/Colab Notebooks/HW4&Final/FINAL\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC9tGDj9egAn",
        "outputId": "00b3a323-861e-40e2-fb42-e5eb363ca696"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 替换以下路径为您数据集的实际路径\n",
        "file_path = 'AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt'\n",
        "\n",
        "import chardet\n",
        "\n",
        "# 打开文件，读取一部分数据来检测编码\n",
        "with open(file_path, 'rb') as file:\n",
        "    raw_data = file.read(10000)  # 读取文件的前10000字节\n",
        "    encoding_result = chardet.detect(raw_data)\n",
        "    file_encoding = encoding_result['encoding']\n",
        "    print(f\"Detected encoding: {file_encoding}\")\n",
        "\n",
        "# 使用检测到的编码重新打开文件\n",
        "try:\n",
        "    with open(file_path, 'r', encoding=file_encoding) as file:\n",
        "        content = file.readlines()\n",
        "    print(\"File read successfully with detected encoding.\")\n",
        "except UnicodeDecodeError as e:\n",
        "    print(f\"UnicodeDecodeError with {file_encoding}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1WY_eHddltQ",
        "outputId": "45de64dd-851d-4996-9bc9-178329b59aeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected encoding: ascii\n",
            "UnicodeDecodeError with ascii: 'ascii' codec can't decode byte 0x96 in position 1614: ordinal not in range(128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    content = file.readlines()\n",
        "print(\"File read with ignoring errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd0Pfpw2glXB",
        "outputId": "040347e6-63f8-43fe-da7b-065a451f1764"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File read with ignoring errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = []\n",
        "for item in content:\n",
        "    split_item = item.split('|')\n",
        "    data.append((split_item[0], split_item[1], split_item[-1]))\n",
        "print (data[0:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m12wFGL1Cvyf",
        "outputId": "1f7ede99-8aee-49fb-b86d-7c3c8e2ce02d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('AB', 'abortion', '_%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida 3, para 2-0-0-2, who presented to the Emergency Room complaining of increasing vaginal bleeding since approximately 6 a.m. The patient does have a known history of having had a missed AB. She had been followed at another clinic and was told that she had a missed AB shortly after Christmas. The patient at that time had been counseled to undergo a D&C and was even offered misoprostol to help complete a miscarriage, however, patient declined at that time to schedule a D&C or to take the misoprostol.\\n'), ('AB', 'abortion', 'She is now bleeding quite heavily. Ultrasound this morning demonstrated a missed AB consistent with a 6 week pregnancy with blood clots in the uterine cavity, as well as continued bleeding from the cervical os. This is consistent with an incomplete AB. The patient presents now for a suction D&C. Medical history is negative. Surgical history is negative. CURRENT MEDICATIONS: Include prenatal vitamins.\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "data2_acronym_dict = Counter([item[0] for item in data])\n",
        "# print (data2_acronym_dict)\n",
        "print (len(data2_acronym_dict))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-w4XH8OEdUf",
        "outputId": "abd2febb-2276-4625-a0b6-3b69b0dc6f8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_sense_dict = {}\n",
        "for item in data:\n",
        "    if word_sense_dict.get(item[0]):\n",
        "        word_sense_dict[item[0]][item[1]] += 1\n",
        "    else:\n",
        "        word_sense_dict[item[0]] = Counter()\n",
        "        word_sense_dict[item[0]][item[1]] = 1\n",
        "print (word_sense_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9-rwjhzE1si",
        "outputId": "fb8f5932-dd1b-4f89-84cd-7f0283160574"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'AB': Counter({'abortion': 345, 'blood group in ABO system': 137, 'type A, type B': 8, 'atrioventricular:AV': 2, 'ankle-brachial': 1, 'arteriovenous:AV': 1, 'X-ray finding': 1, 'MISTAKE:abduction': 1, 'antipyrine benzocaine': 1, 'arterial blood': 1, 'UNSURED SENSE': 1, 'NAME': 1}), 'VBG': Counter({'vertical banded gastroplasty': 299, 'venous blood gas': 201}), 'AC': Counter({'(drug) AC': 161, 'acromioclavicular': 158, 'adriamycin cyclophosphamide': 118, 'before meals': 42, 'assist control': 9, 'acetate': 4, 'angiotensin-converting enzyme:ACE': 3, 'abdominal circumference': 2, 'alternating current': 1, 'anticoagulation': 1, 'antecubital': 1}), 'ALD': Counter({'ad lib on demand': 407, 'adrenoleukodystrophy': 88, 'alanine aminotransferase:ALT': 3, 'left anterior descending:LAD': 1, 'acetyl lysergic acid diethylamide': 1}), 'AMA': Counter({'against medical advice': 444, 'advanced maternal age': 31, 'antimitochondrial antibody': 25}), 'ASA': Counter({'acetylsalicylic acid': 404, 'American Society of Anesthesiologists': 93, 'aminosalicylic acid': 3}), 'AVR': Counter({'aortic valve replacement': 381, 'augmented voltage right arm': 103, 'aortic valve regurgitation': 5, 'aortic valve resistance': 4, 'rapid ventricular response:RVR': 4, 'UNSURED SENSE': 2, 'auditory brainstem response:ABR': 1}), 'AV': Counter({'atrioventricular': 374, 'arteriovenous': 116, 'aortic valve': 8, 'UNSURED SENSE': 2}), 'BAL': Counter({'bronchoalveolar lavage': 457, 'blood alcohol level': 43}), 'BK': Counter({'BK (virus)': 343, 'below knee': 157}), 'BMP': Counter({'basic metabolic profile': 456, 'beta-natriuretic peptide:BNP': 36, 'bone morphogenetic protein': 7, 'bone marrow transplant:BMT': 1}), 'BM': Counter({'bowel movement': 459, 'breast milk': 25, 'bone marrow': 14, 'UNSURED SENSE': 2}), 'C&S': Counter({'conjunctivae and sclerae': 434, 'culture and sensitivity': 47, 'protein C and protein S': 16, 'central nervous system:CNS': 2, 'carcinosarcoma:CaS': 1}), 'C3': Counter({'cervical (level) 3': 249, '(complement) component 3': 243, 'propionylcarnitine': 6, '(stage) C3': 2}), 'C4': Counter({'cervical (level) 4': 261, '(complement) component 4': 231, 'cluster of differentiation 4:CD4': 6, 'fourth heart sound:S4': 1, '(PO Box) C4': 1}), 'CA': Counter({'cancer': 391, 'carbohydrate antigen': 105, 'California': 2, 'UNSURED SENSE': 2}), 'CDI': Counter({\"Children's Depression Inventory\": 270, 'center for diagnostic imaging': 225, 'clean, dry, intact': 3, 'UNSURED SENSE': 2}), 'CEA': Counter({'carcinoembryonic antigen': 444, 'carotid endarterectomy': 53, 'cerebrovascular accident:CVA': 1, 'cancer:CA': 1, 'UNSURED SENSE': 1}), 'CR': Counter({'controlled release': 453, 'cardiorespiratory': 28, 'complete remission': 16, 'C-reactive': 1, 'creatinine': 1, 'closed reduction': 1}), 'CTA': Counter({'clear to auscultation': 396, 'computed tomographic angiography': 100, 'UNSURED SENSE': 2, 'cerebellopontine angle:CPA': 1, 'creatine phosphokinase:CPK': 1}), 'CVA': Counter({'cerebrovascular accident': 278, 'costovertebral angle': 222}), 'CVP': Counter({'central venous pressure': 436, 'cyclophosphamide, vincristine, prednisone': 62, 'cardiovascular pulmonary': 2}), 'CVS': Counter({'chorionic villus sampling': 457, 'cardiovascular system': 41, 'customer, value, service': 2}), 'DC': Counter({'discontinue': 282, 'direct current': 152, 'discharge': 31, 'District of Columbia': 31, '(diltiazem) DC': 1, 'deceased donor:DD': 1, 'direct and consensual': 1, '(drug) DC': 1}), 'DIP': Counter({'distal interphalangeal': 462, 'desquamative interstitial pneumonia': 36, 'dipropionate': 2}), 'DM': Counter({'dextromethorphan': 286, 'diabetes mellitus': 209, 'UNSURED SENSE': 3, 'medical doctor:MD': 1, 'NAME': 1}), 'DT': Counter({'diphtheria-tetanus': 336, 'delirium tremens': 129, 'dorsalis pedis:DP': 23, 'UNSURED SENSE': 4, '(drug) DT': 3, 'deep vein thrombosis:DVT': 3, 'physical therapy:PT': 1, 'doppler echo:DE': 1}), 'EC': Counter({'enteric-coated': 439, 'enterocutaneous': 45, 'UNSURED SENSE': 11, 'epirubicin': 2, 'extensor carpi': 2, 'MISTAKE:EZ PAP': 1}), 'ER': Counter({'emergency room': 448, 'extended release': 34, 'estrogen receptor': 18}), 'ES': Counter({'extra strength': 469, 'enhanced sensitivity': 14, 'ejection fraction:EF': 8, 'UNSURED SENSE': 7, '(drug) ES': 1, 'erythrocyte sedimentation rate:ESR': 1}), 'ET': Counter({'enterostomal therapy': 289, 'endotracheal': 200, 'electrophysiology:EP': 6, 'elective termination': 1, 'UNSURED SENSE': 1, 'enterocutaneous:EC': 1, 'electroconvulsive therapy:ECT': 1, 'pressure equalization:PE': 1}), 'FISH': Counter({'fluorescent in situ hybridization': 449, 'GENERAL ENGLISH': 51}), 'FSH': Counter({'follicle-stimulating hormone': 265, 'Fairview Southdale Hospital': 231, 'fascioscapulohumeral muscular dystrophy': 4}), 'GT': Counter({'gastrostomy tube': 446, 'glutamyl transpeptidase': 30, 'gutta': 16, 'gamma-glutamyltransferase:GGT': 5, 'glucose tolerance': 2, 'guttae:GGT': 1}), 'IA': Counter({'(stage) IA': 275, 'intraarterial': 176, 'Iowa': 19, '(grade) IA': 11, '(type) IA': 5, '(status) IA': 5, 'UNSURED SENSE': 4, '(class) IA': 3, 'transient ischemic attack:TIA': 2}), 'IB': Counter({'(stage) IB': 472, '(grade) IB': 8, '(status) IB': 8, 'international baccalaureate': 5, '(cycle) IB': 2, '(type) IB': 2, 'interferon beta': 1, 'UNSURED SENSE': 1, 'intravenous:IV': 1}), 'IM': Counter({'intramuscular': 461, 'intramedullary': 38, 'UNSURED SENSE': 1}), 'IR': Counter({'interventional radiology': 394, 'immediate-release': 102, 'internal rotation': 2, 'UNSURED SENSE': 1, 'infrared': 1}), 'ITP': Counter({'idiopathic thrombocytopenic purpura': 500}), 'IT': Counter({'GENERAL ENGLISH': 225, 'information technology': 103, 'intrathecal': 58, 'ischial tuberosity': 48, 'iliotibial': 40, 'intertrochanteric': 14, 'UNSURED SENSE': 6, '(drug) IT': 2, 'pravastatin or atorvastatin evaluation and infection therapy': 1, 'inspiratory time': 1, 'immature-to-total neutrophil': 1, 'idiopathic thrombocytopenic purpura:ITP': 1}), 'IVF': Counter({'in vitro fertilization': 308, 'intravenous fluid': 188, 'UNSURED SENSE': 3, 'inferior vena cava:IVC': 1}), 'LA': Counter({'long-acting': 426, 'Los Angeles': 40, 'left atrial': 30, 'Louisiana': 2, 'UNSURED SENSE': 1, 'antinuclear antibody:ANA': 1}), 'LE': Counter({'leukocyte esterase': 345, 'lower extremity': 134, 'UNSURED SENSE': 5, 'left ventricle:LV': 5, 'lupus erythematosus': 3, 'lymphedema': 3, 'NAME': 2, 'long-acting:LA': 2, 'sinemet-levodopa': 1}), 'MOM': Counter({'multiples of median': 439, 'milk of magnesia': 57, 'GENERAL ENGLISH': 3, 'Mall of America:MOA': 1}), 'MP': Counter({'metacarpophalangeal': 179, 'mercaptopurine': 107, 'metatarsophalangeal/metacarpophalangeal': 105, 'metatarsophalangeal': 55, 'metabolic panel': 12, 'nurse practitioner:NP': 11, 'UNSURED SENSE': 11, 'metarsophalangeal': 6, '(device) MP': 5, 'military police': 4, '(drug) MP': 2, 'menstrual period': 1, 'milligram:mg': 1, 'mesangial proliferative': 1}), 'MR': Counter({'magnetic resonance': 314, 'mitral regurgitation': 176, 'GENERAL ENGLISH': 5, 'mental retardation': 3, 'medical record': 1, 'myocardial infarction:MI': 1}), 'MSSA': Counter({'modified selective severity assessment': 418, 'methicillin-susceptible Staphylococcus aureus': 82}), 'MS': Counter({'morphine sulfate': 279, 'multiple sclerosis': 207, 'UNSURED SENSE': 4, 'master of science': 3, 'musculoskeletal': 2, 'medical student': 1, '(drug) MS': 1, 'mitral stenosis': 1, 'GENERAL ENGLISH': 1, 'NAME': 1}), 'NAD': Counter({'no acute distress': 377, 'nothing abnormal detected': 123}), 'NA': Counter({'Narcotics Anonymous': 474, 'sodium': 14, 'not applicable': 10, 'deoxyribonucleic acid:DNA': 1, 'nurse anesthetist': 1}), 'NP': Counter({'nurse practitioner': 438, 'nasopharyngeal': 53, 'UNSURED SENSE': 5, 'nasopharynx': 2, 'natriuretic peptide': 1, '(drug) NP': 1}), 'OP': Counter({'oropharynx': 308, 'oblique presentation/occiput posterior': 121, 'operative': 55, 'UNSURED SENSE': 6, 'ophthalmic': 5, 'occiput posterior': 3, 'ova and parasites': 1, 'outpatient': 1}), 'OR': Counter({'operating room': 466, 'GENERAL ENGLISH': 32, '(drug) OR': 1, 'UNSURED SENSE': 1}), 'OTC': Counter({'over the counter': 469, 'ornithine transcarbamoylase': 31}), 'PAC': Counter({'premature atrial contraction': 275, 'physician assistant certification': 137, 'post anesthesia care': 47, 'picture archiving communication': 25, 'patient-controlled analgesia:PCA': 7, 'UNSURED SENSE': 4, '(drug) PAC': 2, 'prostate-specific antigen:PSA': 1, 'pulmonary arterial concentration': 1, 'pulmonary artery catheter': 1}), 'PA': Counter({'posterior-anterior': 212, 'pulmonary artery': 138, 'physician associates': 83, 'physician assistant': 61, 'UNSURED SENSE': 2, 'tissue plasminogen activator:TPA': 2, 'pulmonary auscultation': 1, 'pulmonary embolus:PE': 1}), 'PCP': Counter({'Pneumocystis jiroveci pneumonia': 294, 'primary care physician': 111, 'phencyclidine': 93, 'patient-controlled analgesia:PCA': 1, 'UNSURED SENSE': 1}), 'PDA': Counter({'posterior descending artery': 361, 'patent ductus arteriosus': 138, 'patient-controlled analgesia:PCA': 1}), 'PD': Counter({'peritoneal dialysis': 409, 'posterior descending': 34, 'police department': 14, 'phosphate dehydrogenase': 9, 'pancreatic duct': 8, '(device) PD': 6, 'UNSURED SENSE': 6, '(drug) PD': 3, 'pulmonary embolus:PE': 3, 'prism diopter': 3, 'patent ductus': 1, 'dorsalis pedis:DP': 1, 'Parkinson disease': 1, 'personality disorder': 1, 'purified protein derivative:PPD': 1}), 'PE': Counter({'pulmonary embolus': 408, 'pressure equalization': 89, 'UNSURED SENSE': 2, 'pleural effusion': 1}), 'PM': Counter({'afternoon': 423, 'physical medicine and rehabilitation:PMR': 74, 'UNSURED SENSE': 2, 'metacarpophalangeal:MP': 1}), 'PR': Counter({'pr interval': 252, 'per rectum': 141, 'progesterone receptor': 88, 'pulmonary regurgitation': 12, 'UNSURED SENSE': 4, '(drug) PR': 2, 'pulse rate': 1}), 'PT': Counter({'physical therapy': 455, 'prothrombin time': 22, 'posterior tibial': 21, 'prothrombin': 1, 'UNSURED SENSE': 1}), 'RA': Counter({'right atrium': 394, 'rheumatoid arthritis': 66, 'room air': 36, 'UNSURED SENSE': 3, 'retinoic acid': 1}), 'RT': Counter({'radiation therapy': 336, 'respiratory therapy': 149, 'retrograde tachycardia': 7, 'respiratory therapist': 2, 'UNSURED SENSE': 2, 'NAME': 2, 'right': 1, '(drug) RT': 1}), 'SA': Counter({'slow acting/sustained action': 373, 'sinuatrial': 88, 'UNSURED SENSE': 29, 'saturation': 4, 'sinus arrest': 2, 'MISTAKE:Oncotype DX': 2, 'methicillin-susceptible Staphylococcus aureus:MSSA': 1, 'American Society of Anesthesiologists:ASA': 1}), 'SBP': Counter({'spontaneous bacterial peritonitis': 417, 'systolic blood pressure': 83}), 'SMA': Counter({'superior mesenteric artery': 353, 'sequential multiple autoanalyzer': 84, 'spinal muscular atrophy': 56, 'smooth muscle antibody': 3, 'smooth muscle actin': 2, 'UNSURED SENSE': 2}), 'SS': Counter({'single strength': 439, 'UNSURED SENSE': 57, 'sickle cell genotype SS': 4}), 'T1': Counter({'tumor stage 1': 198, 'thoracic (level) 1': 194, 'T1 (MRI)': 103, 'UNSURED SENSE': 3, 'term 1': 1, 'type 1 (diabetes mellitus)': 1}), 'T2': Counter({'T2 (MRI)': 227, 'tumor stage 2': 166, 'thoracic (level) 2': 97, 'UNSURED SENSE': 7, 'term 2': 1, 'T2 Nodes': 1, 'S2 (heart sound):S2': 1}), 'T3': Counter({'triiodothyronine': 268, 'tumor stage 3': 156, 'thoracic (level) 3': 65, 'UNSURED SENSE': 5, 'T3 (ECG pattern)': 4, 'term 3': 2}), 'T4': Counter({'thyroxine': 424, 'thoracic (level) 4': 41, 'tumor stage 4': 35}), 'US': Counter({'United States': 402, 'ultrasound': 94, 'GENERAL ENGLISH': 3, 'UNSURED SENSE': 1}), 'VAD': Counter({'vincristine adriamycin and dexamethasone': 396, 'ventricular assist device': 87, 'vascular access device': 13, 'UNSURED SENSE': 3, 'video-assisted thoracic surgery:VATS': 1})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_word_sense_per_acronym = []\n",
        "for key, value in word_sense_dict.items():\n",
        "    no_word_sense_per_acronym.append(len(value))\n",
        "no_word_sense_per_acronym = Counter(no_word_sense_per_acronym)\n",
        "print (no_word_sense_per_acronym)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo810CH0Fe9P",
        "outputId": "53cdb13c-b226-4523-e593-443bcfceb8e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({5: 12, 4: 12, 3: 11, 6: 10, 2: 9, 8: 7, 7: 3, 9: 3, 12: 2, 10: 2, 11: 1, 1: 1, 14: 1, 15: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_sentences_per_word_sense = []\n",
        "for acronym, value in word_sense_dict.items():\n",
        "    for word_sense, no_sentences in value.items():\n",
        "        no_sentences_per_word_sense.append(no_sentences)\n",
        "no_sentences_per_word_sense = Counter(no_sentences_per_word_sense)\n",
        "print (no_sentences_per_word_sense)\n",
        "sentence_len_per_word_sense = Counter([len(item[2]) for item in data])\n",
        "# print (data2_sentence_dict)\n",
        "print (f\"max length:{max(sentence_len_per_word_sense.keys())}; min length:{min(sentence_len_per_word_sense.keys())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxG_O4n0UjO8",
        "outputId": "6d3961c1-28cb-464a-bd88-0095d3b5d98f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 106, 2: 42, 3: 23, 4: 13, 5: 12, 6: 8, 8: 6, 7: 5, 14: 5, 31: 4, 11: 4, 88: 3, 25: 3, 103: 3, 36: 3, 16: 3, 439: 3, 345: 2, 137: 2, 9: 2, 444: 2, 93: 2, 457: 2, 47: 2, 231: 2, 105: 2, 225: 2, 53: 2, 396: 2, 41: 2, 336: 2, 34: 2, 469: 2, 30: 2, 176: 2, 275: 2, 394: 2, 40: 2, 308: 2, 57: 2, 55: 2, 12: 2, 138: 2, 83: 2, 299: 1, 201: 1, 161: 1, 158: 1, 118: 1, 42: 1, 407: 1, 404: 1, 381: 1, 374: 1, 116: 1, 43: 1, 343: 1, 157: 1, 456: 1, 459: 1, 434: 1, 249: 1, 243: 1, 261: 1, 391: 1, 270: 1, 453: 1, 28: 1, 100: 1, 222: 1, 278: 1, 62: 1, 436: 1, 152: 1, 282: 1, 462: 1, 286: 1, 209: 1, 129: 1, 23: 1, 45: 1, 448: 1, 18: 1, 289: 1, 200: 1, 449: 1, 51: 1, 265: 1, 446: 1, 19: 1, 472: 1, 461: 1, 38: 1, 102: 1, 500: 1, 58: 1, 48: 1, 188: 1, 426: 1, 134: 1, 107: 1, 179: 1, 314: 1, 418: 1, 82: 1, 207: 1, 279: 1, 123: 1, 377: 1, 474: 1, 10: 1, 438: 1, 121: 1, 466: 1, 32: 1, 212: 1, 61: 1, 111: 1, 294: 1, 361: 1, 409: 1, 408: 1, 89: 1, 423: 1, 74: 1, 141: 1, 252: 1, 455: 1, 22: 1, 21: 1, 66: 1, 149: 1, 373: 1, 29: 1, 417: 1, 353: 1, 84: 1, 56: 1, 198: 1, 194: 1, 166: 1, 227: 1, 97: 1, 65: 1, 268: 1, 156: 1, 424: 1, 35: 1, 402: 1, 94: 1, 87: 1, 13: 1})\n",
            "max length:2708; min length:93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 加载预训练的BioBERT分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "\n",
        "class ClinicalData(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 将模型需要的tensor打包成字典返回\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # 确保标签是长整型\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyIFRbEZRuZy",
        "outputId": "491c00f3-fb5d-4263-c5a2-08473ee57a6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印前几个数据点查看结构\n",
        "for i in range(5):\n",
        "    print(data[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd5YvFVWq8HI",
        "outputId": "0684abc4-fc66-4063-dff1-d2d60aff8eb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('AB', 'abortion', '_%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida 3, para 2-0-0-2, who presented to the Emergency Room complaining of increasing vaginal bleeding since approximately 6 a.m. The patient does have a known history of having had a missed AB. She had been followed at another clinic and was told that she had a missed AB shortly after Christmas. The patient at that time had been counseled to undergo a D&C and was even offered misoprostol to help complete a miscarriage, however, patient declined at that time to schedule a D&C or to take the misoprostol.\\n')\n",
            "('AB', 'abortion', 'She is now bleeding quite heavily. Ultrasound this morning demonstrated a missed AB consistent with a 6 week pregnancy with blood clots in the uterine cavity, as well as continued bleeding from the cervical os. This is consistent with an incomplete AB. The patient presents now for a suction D&C. Medical history is negative. Surgical history is negative. CURRENT MEDICATIONS: Include prenatal vitamins.\\n')\n",
            "('AB', 'abortion', 'ALLERGIES: Heparin and Imitrex. PAST OB HISTORY: 1. 1992 full term primary section for breech presentation. 2. 1995 full term successful VBAC, no complications. 3. _%#1999#%_ full term repeat C-section. 4. 2005 spontaneous AB followed by suction D&C. PAST GYN HISTORY: The patient denies any history of abnormal Pap smears except for her most recent Pap performed _%#MM2006#%_.\\n')\n",
            "('AB', 'abortion', 'She had a pelvic ultrasound at Park Nicollet on _%#MMDD#%_ that showed six weeks, six days gestation sac which is empty with no pole or cardiac activity consistent with blighted ovum and missed AB. The patient waited and did not miscarry on her own and she desires suction curettage of uterus. She is scheduled to undergo suction curettage on _%#MMDD2007#%_ under general anesthesia.\\n')\n",
            "('AB', 'abortion', 'On _%#MMDD2007#%_, normal anatomy with anterior placenta. No evidence of previa. PAST OB-GYN HISTORY: 1. Elective AB in the first trimester x 2. 2. NSVD at 36 weeks in 1996. 3. No STIs or abnormal Pap smears. PAST MEDICAL HISTORY: None. MEDICATIONS: None. ALLERGIES: NKDA. SOCIAL HISTORY: No tobacco, alcohol or drugs.\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 假设`data`是一个列表，每个元素是一个包含(text, label)的元组\n",
        "# 从数据中提取标签和文本\n",
        "labels, texts = zip(*[(item[1], item[2]) for item in data])\n",
        "\n",
        "# 清理非字符串类型的标签\n",
        "labels = [label for label in labels if isinstance(label, str)]\n",
        "\n",
        "# 现在尝试创建映射\n",
        "label_map = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
        "\n",
        "# 显示标签映射以确保其正确性\n",
        "print(\"标签映射:\", label_map)\n",
        "\n",
        "\n",
        "# 将字符串标签转换为整数索引\n",
        "labels_indexed = [label_map[label] for label in labels]\n",
        "\n",
        "# 显示转换后的整数标签示例以确保其正确性\n",
        "print(\"转换后的整数标签示例:\", labels_indexed[:5])\n",
        "\n",
        "# 首先分割训练集和剩余部分\n",
        "texts_train, texts_temp, labels_train, labels_temp = train_test_split(\n",
        "    texts, labels_indexed, test_size=0.2, random_state=42)\n",
        "\n",
        "# 然后将剩余部分分割为验证集和测试集\n",
        "texts_val, texts_test, labels_val, labels_test = train_test_split(\n",
        "    texts_temp, labels_temp, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTmBSr_4n7wQ",
        "outputId": "c6ff5c1d-241b-41d3-dd5e-091954f55aa4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "标签映射: {'(PO Box) C4': 0, '(class) IA': 1, '(complement) component 3': 2, '(complement) component 4': 3, '(cycle) IB': 4, '(device) MP': 5, '(device) PD': 6, '(diltiazem) DC': 7, '(drug) AC': 8, '(drug) DC': 9, '(drug) DT': 10, '(drug) ES': 11, '(drug) IT': 12, '(drug) MP': 13, '(drug) MS': 14, '(drug) NP': 15, '(drug) OR': 16, '(drug) PAC': 17, '(drug) PD': 18, '(drug) PR': 19, '(drug) RT': 20, '(grade) IA': 21, '(grade) IB': 22, '(stage) C3': 23, '(stage) IA': 24, '(stage) IB': 25, '(status) IA': 26, '(status) IB': 27, '(type) IA': 28, '(type) IB': 29, 'American Society of Anesthesiologists': 30, 'American Society of Anesthesiologists:ASA': 31, 'BK (virus)': 32, 'C-reactive': 33, 'California': 34, \"Children's Depression Inventory\": 35, 'District of Columbia': 36, 'Fairview Southdale Hospital': 37, 'GENERAL ENGLISH': 38, 'Iowa': 39, 'Los Angeles': 40, 'Louisiana': 41, 'MISTAKE:EZ PAP': 42, 'MISTAKE:Oncotype DX': 43, 'MISTAKE:abduction': 44, 'Mall of America:MOA': 45, 'NAME': 46, 'Narcotics Anonymous': 47, 'Parkinson disease': 48, 'Pneumocystis jiroveci pneumonia': 49, 'S2 (heart sound):S2': 50, 'T1 (MRI)': 51, 'T2 (MRI)': 52, 'T2 Nodes': 53, 'T3 (ECG pattern)': 54, 'UNSURED SENSE': 55, 'United States': 56, 'X-ray finding': 57, 'abdominal circumference': 58, 'abortion': 59, 'acetate': 60, 'acetyl lysergic acid diethylamide': 61, 'acetylsalicylic acid': 62, 'acromioclavicular': 63, 'ad lib on demand': 64, 'adrenoleukodystrophy': 65, 'adriamycin cyclophosphamide': 66, 'advanced maternal age': 67, 'afternoon': 68, 'against medical advice': 69, 'alanine aminotransferase:ALT': 70, 'alternating current': 71, 'aminosalicylic acid': 72, 'angiotensin-converting enzyme:ACE': 73, 'ankle-brachial': 74, 'antecubital': 75, 'anticoagulation': 76, 'antimitochondrial antibody': 77, 'antinuclear antibody:ANA': 78, 'antipyrine benzocaine': 79, 'aortic valve': 80, 'aortic valve regurgitation': 81, 'aortic valve replacement': 82, 'aortic valve resistance': 83, 'arterial blood': 84, 'arteriovenous': 85, 'arteriovenous:AV': 86, 'assist control': 87, 'atrioventricular': 88, 'atrioventricular:AV': 89, 'auditory brainstem response:ABR': 90, 'augmented voltage right arm': 91, 'basic metabolic profile': 92, 'before meals': 93, 'below knee': 94, 'beta-natriuretic peptide:BNP': 95, 'blood alcohol level': 96, 'blood group in ABO system': 97, 'bone marrow': 98, 'bone marrow transplant:BMT': 99, 'bone morphogenetic protein': 100, 'bowel movement': 101, 'breast milk': 102, 'bronchoalveolar lavage': 103, 'cancer': 104, 'cancer:CA': 105, 'carbohydrate antigen': 106, 'carcinoembryonic antigen': 107, 'carcinosarcoma:CaS': 108, 'cardiorespiratory': 109, 'cardiovascular pulmonary': 110, 'cardiovascular system': 111, 'carotid endarterectomy': 112, 'center for diagnostic imaging': 113, 'central nervous system:CNS': 114, 'central venous pressure': 115, 'cerebellopontine angle:CPA': 116, 'cerebrovascular accident': 117, 'cerebrovascular accident:CVA': 118, 'cervical (level) 3': 119, 'cervical (level) 4': 120, 'chorionic villus sampling': 121, 'clean, dry, intact': 122, 'clear to auscultation': 123, 'closed reduction': 124, 'cluster of differentiation 4:CD4': 125, 'complete remission': 126, 'computed tomographic angiography': 127, 'conjunctivae and sclerae': 128, 'controlled release': 129, 'costovertebral angle': 130, 'creatine phosphokinase:CPK': 131, 'creatinine': 132, 'culture and sensitivity': 133, 'customer, value, service': 134, 'cyclophosphamide, vincristine, prednisone': 135, 'deceased donor:DD': 136, 'deep vein thrombosis:DVT': 137, 'delirium tremens': 138, 'deoxyribonucleic acid:DNA': 139, 'desquamative interstitial pneumonia': 140, 'dextromethorphan': 141, 'diabetes mellitus': 142, 'diphtheria-tetanus': 143, 'dipropionate': 144, 'direct and consensual': 145, 'direct current': 146, 'discharge': 147, 'discontinue': 148, 'distal interphalangeal': 149, 'doppler echo:DE': 150, 'dorsalis pedis:DP': 151, 'ejection fraction:EF': 152, 'elective termination': 153, 'electroconvulsive therapy:ECT': 154, 'electrophysiology:EP': 155, 'emergency room': 156, 'endotracheal': 157, 'enhanced sensitivity': 158, 'enteric-coated': 159, 'enterocutaneous': 160, 'enterocutaneous:EC': 161, 'enterostomal therapy': 162, 'epirubicin': 163, 'erythrocyte sedimentation rate:ESR': 164, 'estrogen receptor': 165, 'extended release': 166, 'extensor carpi': 167, 'extra strength': 168, 'fascioscapulohumeral muscular dystrophy': 169, 'fluorescent in situ hybridization': 170, 'follicle-stimulating hormone': 171, 'fourth heart sound:S4': 172, 'gamma-glutamyltransferase:GGT': 173, 'gastrostomy tube': 174, 'glucose tolerance': 175, 'glutamyl transpeptidase': 176, 'gutta': 177, 'guttae:GGT': 178, 'idiopathic thrombocytopenic purpura': 179, 'idiopathic thrombocytopenic purpura:ITP': 180, 'iliotibial': 181, 'immature-to-total neutrophil': 182, 'immediate-release': 183, 'in vitro fertilization': 184, 'inferior vena cava:IVC': 185, 'information technology': 186, 'infrared': 187, 'inspiratory time': 188, 'interferon beta': 189, 'internal rotation': 190, 'international baccalaureate': 191, 'intertrochanteric': 192, 'interventional radiology': 193, 'intraarterial': 194, 'intramedullary': 195, 'intramuscular': 196, 'intrathecal': 197, 'intravenous fluid': 198, 'intravenous:IV': 199, 'ischial tuberosity': 200, 'left anterior descending:LAD': 201, 'left atrial': 202, 'left ventricle:LV': 203, 'leukocyte esterase': 204, 'long-acting': 205, 'long-acting:LA': 206, 'lower extremity': 207, 'lupus erythematosus': 208, 'lymphedema': 209, 'magnetic resonance': 210, 'master of science': 211, 'medical doctor:MD': 212, 'medical record': 213, 'medical student': 214, 'menstrual period': 215, 'mental retardation': 216, 'mercaptopurine': 217, 'mesangial proliferative': 218, 'metabolic panel': 219, 'metacarpophalangeal': 220, 'metacarpophalangeal:MP': 221, 'metarsophalangeal': 222, 'metatarsophalangeal': 223, 'metatarsophalangeal/metacarpophalangeal': 224, 'methicillin-susceptible Staphylococcus aureus': 225, 'methicillin-susceptible Staphylococcus aureus:MSSA': 226, 'military police': 227, 'milk of magnesia': 228, 'milligram:mg': 229, 'mitral regurgitation': 230, 'mitral stenosis': 231, 'modified selective severity assessment': 232, 'morphine sulfate': 233, 'multiple sclerosis': 234, 'multiples of median': 235, 'musculoskeletal': 236, 'myocardial infarction:MI': 237, 'nasopharyngeal': 238, 'nasopharynx': 239, 'natriuretic peptide': 240, 'no acute distress': 241, 'not applicable': 242, 'nothing abnormal detected': 243, 'nurse anesthetist': 244, 'nurse practitioner': 245, 'nurse practitioner:NP': 246, 'oblique presentation/occiput posterior': 247, 'occiput posterior': 248, 'operating room': 249, 'operative': 250, 'ophthalmic': 251, 'ornithine transcarbamoylase': 252, 'oropharynx': 253, 'outpatient': 254, 'ova and parasites': 255, 'over the counter': 256, 'pancreatic duct': 257, 'patent ductus': 258, 'patent ductus arteriosus': 259, 'patient-controlled analgesia:PCA': 260, 'per rectum': 261, 'peritoneal dialysis': 262, 'personality disorder': 263, 'phencyclidine': 264, 'phosphate dehydrogenase': 265, 'physical medicine and rehabilitation:PMR': 266, 'physical therapy': 267, 'physical therapy:PT': 268, 'physician assistant': 269, 'physician assistant certification': 270, 'physician associates': 271, 'picture archiving communication': 272, 'pleural effusion': 273, 'police department': 274, 'post anesthesia care': 275, 'posterior descending': 276, 'posterior descending artery': 277, 'posterior tibial': 278, 'posterior-anterior': 279, 'pr interval': 280, 'pravastatin or atorvastatin evaluation and infection therapy': 281, 'premature atrial contraction': 282, 'pressure equalization': 283, 'pressure equalization:PE': 284, 'primary care physician': 285, 'prism diopter': 286, 'progesterone receptor': 287, 'propionylcarnitine': 288, 'prostate-specific antigen:PSA': 289, 'protein C and protein S': 290, 'prothrombin': 291, 'prothrombin time': 292, 'pulmonary arterial concentration': 293, 'pulmonary artery': 294, 'pulmonary artery catheter': 295, 'pulmonary auscultation': 296, 'pulmonary embolus': 297, 'pulmonary embolus:PE': 298, 'pulmonary regurgitation': 299, 'pulse rate': 300, 'purified protein derivative:PPD': 301, 'radiation therapy': 302, 'rapid ventricular response:RVR': 303, 'respiratory therapist': 304, 'respiratory therapy': 305, 'retinoic acid': 306, 'retrograde tachycardia': 307, 'rheumatoid arthritis': 308, 'right': 309, 'right atrium': 310, 'room air': 311, 'saturation': 312, 'sequential multiple autoanalyzer': 313, 'sickle cell genotype SS': 314, 'sinemet-levodopa': 315, 'single strength': 316, 'sinuatrial': 317, 'sinus arrest': 318, 'slow acting/sustained action': 319, 'smooth muscle actin': 320, 'smooth muscle antibody': 321, 'sodium': 322, 'spinal muscular atrophy': 323, 'spontaneous bacterial peritonitis': 324, 'superior mesenteric artery': 325, 'systolic blood pressure': 326, 'term 1': 327, 'term 2': 328, 'term 3': 329, 'thoracic (level) 1': 330, 'thoracic (level) 2': 331, 'thoracic (level) 3': 332, 'thoracic (level) 4': 333, 'thyroxine': 334, 'tissue plasminogen activator:TPA': 335, 'transient ischemic attack:TIA': 336, 'triiodothyronine': 337, 'tumor stage 1': 338, 'tumor stage 2': 339, 'tumor stage 3': 340, 'tumor stage 4': 341, 'type 1 (diabetes mellitus)': 342, 'type A, type B': 343, 'ultrasound': 344, 'vascular access device': 345, 'venous blood gas': 346, 'ventricular assist device': 347, 'vertical banded gastroplasty': 348, 'video-assisted thoracic surgery:VATS': 349, 'vincristine adriamycin and dexamethasone': 350}\n",
            "转换后的整数标签示例: [59, 59, 59, 59, 59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(texts_train), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(list(texts_val), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(texts_test), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "Y2dbH2DeoFWg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "\n",
        "# 更新你的数据集类，使用整数标签\n",
        "class ClinicalDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels  # 这应该是整数列表\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])  # 此处labels已经是整数\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "# 创建数据集实例\n",
        "train_dataset = ClinicalDataset(train_encodings, labels_train)\n",
        "val_dataset = ClinicalDataset(val_encodings, labels_val)\n",
        "test_dataset = ClinicalDataset(test_encodings, labels_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "lWcKQkcFoXyN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载BioBERT模型\n",
        "model = BertForSequenceClassification.from_pretrained('dmis-lab/biobert-v1.1', num_labels=len(set(labels)))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "\n",
        "# 定义训练参数\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # 输出目录保存训练结果\n",
        "    num_train_epochs=3,              # 训练轮数\n",
        "    per_device_train_batch_size=8,   # 每个设备的训练批大小\n",
        "    per_device_eval_batch_size=16,   # 每个设备的评估批大小\n",
        "    warmup_steps=500,                # 预热步骤\n",
        "    weight_decay=0.01,               # 权重衰减，帮助防止过拟合\n",
        "    logging_dir='./logs',            # 日志目录\n",
        "    logging_steps=500,                # 多少步打印一次日志\n",
        "    evaluation_strategy=\"epoch\",     # 每个epoch结束时评估模型性能\n",
        "    save_strategy=\"epoch\",           # 每个epoch保存模型\n",
        "    load_best_model_at_end=True      # 训练结束时载入最佳模型\n",
        ")\n",
        "\n",
        "# 初始化Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                 # 模型实例\n",
        "    args=training_args,          # 训练参数\n",
        "    train_dataset=train_dataset, # 训练数据集\n",
        "    eval_dataset=val_dataset,    # 验证数据集\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj0b5g1IcPph",
        "outputId": "72461c52-2729-4cf4-bcb6-7559253b6ad9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lsodCYB_gOBW",
        "outputId": "cbc042f4-a7ab-4464-888b-cc8f8bfea5cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11250/11250 11:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.573900</td>\n",
              "      <td>0.481794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.335400</td>\n",
              "      <td>0.412641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.208000</td>\n",
              "      <td>0.384810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11250, training_loss=0.7693890496148004, metrics={'train_runtime': 683.5415, 'train_samples_per_second': 131.667, 'train_steps_per_second': 16.458, 'total_flos': 5938549240320000.0, 'train_loss': 0.7693890496148004, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存训练好的模型\n",
        "model_path = \"./biobert_finetuned\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCqSc1oYR1mG",
        "outputId": "8313a43e-f1a4-4478-eb8b-80749775cc95"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./biobert_finetuned/tokenizer_config.json',\n",
              " './biobert_finetuned/special_tokens_map.json',\n",
              " './biobert_finetuned/vocab.txt',\n",
              " './biobert_finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of classes:\", model.config.num_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAeHhQV3mpbv",
        "outputId": "4073d155-ae95-4b9b-a932-44c6d04e6b12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label Map:\", label_map)\n",
        "print(\"Inverse Label Map:\", inverse_label_map)\n",
        "\n",
        "# 检查预测标签索引是否全部在 inverse_label_map 中\n",
        "pred_indices = set(np.argmax(predictions.predictions, axis=-1))\n",
        "missing_indices = pred_indices - set(inverse_label_map.keys())\n",
        "print(\"Missing Indices:\", missing_indices)\n",
        "\n",
        "if missing_indices:\n",
        "    print(\"Warning: There are missing indices in the inverse_label_map!\")\n",
        "else:\n",
        "    print(\"All predicted indices are accounted for in the inverse_label_map.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU_lZUje7bit",
        "outputId": "44d02f13-a37c-46ff-da4c-27b4feef61ee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Map: {0: 'Class_0', 1: 'Class_1', 2: 'Class_2', 3: 'Class_3', 4: 'Class_4', 5: 'Class_5', 6: 'Class_6', 7: 'Class_7', 8: 'Class_8', 9: 'Class_9', 10: 'Class_10', 11: 'Class_11', 12: 'Class_12', 13: 'Class_13', 14: 'Class_14', 15: 'Class_15', 16: 'Class_16', 17: 'Class_17', 18: 'Class_18', 19: 'Class_19', 20: 'Class_20', 21: 'Class_21', 22: 'Class_22', 23: 'Class_23', 24: 'Class_24', 25: 'Class_25', 26: 'Class_26', 27: 'Class_27', 28: 'Class_28', 29: 'Class_29', 30: 'Class_30', 31: 'Class_31', 32: 'Class_32', 33: 'Class_33', 34: 'Class_34', 35: 'Class_35', 36: 'Class_36', 37: 'Class_37', 38: 'Class_38', 39: 'Class_39', 40: 'Class_40', 41: 'Class_41', 42: 'Class_42', 43: 'Class_43', 44: 'Class_44', 45: 'Class_45', 46: 'Class_46', 47: 'Class_47', 48: 'Class_48', 49: 'Class_49', 50: 'Class_50', 51: 'Class_51', 52: 'Class_52', 53: 'Class_53', 54: 'Class_54', 55: 'Class_55', 56: 'Class_56', 57: 'Class_57', 58: 'Class_58', 59: 'Class_59', 60: 'Class_60', 61: 'Class_61', 62: 'Class_62', 63: 'Class_63', 64: 'Class_64', 65: 'Class_65', 66: 'Class_66', 67: 'Class_67', 68: 'Class_68', 69: 'Class_69', 70: 'Class_70', 71: 'Class_71', 72: 'Class_72', 73: 'Class_73', 74: 'Class_74', 75: 'Class_75', 76: 'Class_76', 77: 'Class_77', 78: 'Class_78', 79: 'Class_79', 80: 'Class_80', 81: 'Class_81', 82: 'Class_82', 83: 'Class_83', 84: 'Class_84', 85: 'Class_85', 86: 'Class_86', 87: 'Class_87', 88: 'Class_88', 89: 'Class_89', 90: 'Class_90', 91: 'Class_91', 92: 'Class_92', 93: 'Class_93', 94: 'Class_94', 95: 'Class_95', 96: 'Class_96', 97: 'Class_97', 98: 'Class_98', 99: 'Class_99', 100: 'Class_100', 101: 'Class_101', 102: 'Class_102', 103: 'Class_103', 104: 'Class_104', 105: 'Class_105', 106: 'Class_106', 107: 'Class_107', 108: 'Class_108', 109: 'Class_109', 110: 'Class_110', 111: 'Class_111', 112: 'Class_112', 113: 'Class_113', 114: 'Class_114', 115: 'Class_115', 116: 'Class_116', 117: 'Class_117', 118: 'Class_118', 119: 'Class_119', 120: 'Class_120', 121: 'Class_121', 122: 'Class_122', 123: 'Class_123', 124: 'Class_124', 125: 'Class_125', 126: 'Class_126', 127: 'Class_127', 128: 'Class_128', 129: 'Class_129', 130: 'Class_130', 131: 'Class_131', 132: 'Class_132', 133: 'Class_133', 134: 'Class_134', 135: 'Class_135', 136: 'Class_136', 137: 'Class_137', 138: 'Class_138', 139: 'Class_139', 140: 'Class_140', 141: 'Class_141', 142: 'Class_142', 143: 'Class_143', 144: 'Class_144', 145: 'Class_145', 146: 'Class_146', 147: 'Class_147', 148: 'Class_148', 149: 'Class_149', 150: 'Class_150', 151: 'Class_151', 152: 'Class_152', 153: 'Class_153', 154: 'Class_154', 155: 'Class_155', 156: 'Class_156', 157: 'Class_157', 158: 'Class_158', 159: 'Class_159', 160: 'Class_160', 161: 'Class_161', 162: 'Class_162', 163: 'Class_163', 164: 'Class_164', 165: 'Class_165', 166: 'Class_166', 167: 'Class_167', 168: 'Class_168', 169: 'Class_169', 170: 'Class_170', 171: 'Class_171', 172: 'Class_172', 173: 'Class_173', 174: 'Class_174', 175: 'Class_175', 176: 'Class_176', 177: 'Class_177', 178: 'Class_178', 179: 'Class_179', 180: 'Class_180', 181: 'Class_181', 182: 'Class_182', 183: 'Class_183', 184: 'Class_184', 185: 'Class_185', 186: 'Class_186', 187: 'Class_187', 188: 'Class_188', 189: 'Class_189', 190: 'Class_190', 191: 'Class_191', 192: 'Class_192', 193: 'Class_193', 194: 'Class_194', 195: 'Class_195', 196: 'Class_196', 197: 'Class_197', 198: 'Class_198', 199: 'Class_199', 200: 'Class_200', 201: 'Class_201', 202: 'Class_202', 203: 'Class_203', 204: 'Class_204', 205: 'Class_205', 206: 'Class_206', 207: 'Class_207', 208: 'Class_208', 209: 'Class_209', 210: 'Class_210', 211: 'Class_211', 212: 'Class_212', 213: 'Class_213', 214: 'Class_214', 215: 'Class_215', 216: 'Class_216', 217: 'Class_217', 218: 'Class_218', 219: 'Class_219', 220: 'Class_220', 221: 'Class_221', 222: 'Class_222', 223: 'Class_223', 224: 'Class_224', 225: 'Class_225', 226: 'Class_226', 227: 'Class_227', 228: 'Class_228', 229: 'Class_229', 230: 'Class_230', 231: 'Class_231', 232: 'Class_232', 233: 'Class_233', 234: 'Class_234', 235: 'Class_235', 236: 'Class_236', 237: 'Class_237', 238: 'Class_238', 239: 'Class_239', 240: 'Class_240', 241: 'Class_241', 242: 'Class_242', 243: 'Class_243', 244: 'Class_244', 245: 'Class_245', 246: 'Class_246', 247: 'Class_247', 248: 'Class_248', 249: 'Class_249', 250: 'Class_250', 251: 'Class_251', 252: 'Class_252', 253: 'Class_253', 254: 'Class_254', 255: 'Class_255', 256: 'Class_256', 257: 'Class_257', 258: 'Class_258', 259: 'Class_259', 260: 'Class_260', 261: 'Class_261', 262: 'Class_262', 263: 'Class_263', 264: 'Class_264', 265: 'Class_265', 266: 'Class_266', 267: 'Class_267', 268: 'Class_268', 269: 'Class_269', 270: 'Class_270', 271: 'Class_271', 272: 'Class_272', 273: 'Class_273', 274: 'Class_274', 275: 'Class_275', 276: 'Class_276', 277: 'Class_277', 278: 'Class_278', 279: 'Class_279', 280: 'Class_280', 281: 'Class_281', 282: 'Class_282', 283: 'Class_283', 284: 'Class_284', 285: 'Class_285', 286: 'Class_286', 287: 'Class_287', 288: 'Class_288', 289: 'Class_289', 290: 'Class_290', 291: 'Class_291', 292: 'Class_292', 293: 'Class_293', 294: 'Class_294', 295: 'Class_295', 296: 'Class_296', 297: 'Class_297', 298: 'Class_298', 299: 'Class_299', 300: 'Class_300', 301: 'Class_301', 302: 'Class_302', 303: 'Class_303', 304: 'Class_304', 305: 'Class_305', 306: 'Class_306', 307: 'Class_307', 308: 'Class_308', 309: 'Class_309', 310: 'Class_310', 311: 'Class_311', 312: 'Class_312', 313: 'Class_313', 314: 'Class_314', 315: 'Class_315', 316: 'Class_316', 317: 'Class_317', 318: 'Class_318', 319: 'Class_319', 320: 'Class_320', 321: 'Class_321', 322: 'Class_322', 323: 'Class_323', 324: 'Class_324', 325: 'Class_325', 326: 'Class_326', 327: 'Class_327', 328: 'Class_328', 329: 'Class_329', 330: 'Class_330', 331: 'Class_331', 332: 'Class_332', 333: 'Class_333', 334: 'Class_334', 335: 'Class_335', 336: 'Class_336', 337: 'Class_337', 338: 'Class_338', 339: 'Class_339', 340: 'Class_340', 341: 'Class_341', 342: 'Class_342', 343: 'Class_343', 344: 'Class_344', 345: 'Class_345', 346: 'Class_346', 347: 'Class_347', 348: 'Class_348', 349: 'Class_349', 350: 'Class_350'}\n",
            "Inverse Label Map: {'Class_0': 0, 'Class_1': 1, 'Class_2': 2, 'Class_3': 3, 'Class_4': 4, 'Class_5': 5, 'Class_6': 6, 'Class_7': 7, 'Class_8': 8, 'Class_9': 9, 'Class_10': 10, 'Class_11': 11, 'Class_12': 12, 'Class_13': 13, 'Class_14': 14, 'Class_15': 15, 'Class_16': 16, 'Class_17': 17, 'Class_18': 18, 'Class_19': 19, 'Class_20': 20, 'Class_21': 21, 'Class_22': 22, 'Class_23': 23, 'Class_24': 24, 'Class_25': 25, 'Class_26': 26, 'Class_27': 27, 'Class_28': 28, 'Class_29': 29, 'Class_30': 30, 'Class_31': 31, 'Class_32': 32, 'Class_33': 33, 'Class_34': 34, 'Class_35': 35, 'Class_36': 36, 'Class_37': 37, 'Class_38': 38, 'Class_39': 39, 'Class_40': 40, 'Class_41': 41, 'Class_42': 42, 'Class_43': 43, 'Class_44': 44, 'Class_45': 45, 'Class_46': 46, 'Class_47': 47, 'Class_48': 48, 'Class_49': 49, 'Class_50': 50, 'Class_51': 51, 'Class_52': 52, 'Class_53': 53, 'Class_54': 54, 'Class_55': 55, 'Class_56': 56, 'Class_57': 57, 'Class_58': 58, 'Class_59': 59, 'Class_60': 60, 'Class_61': 61, 'Class_62': 62, 'Class_63': 63, 'Class_64': 64, 'Class_65': 65, 'Class_66': 66, 'Class_67': 67, 'Class_68': 68, 'Class_69': 69, 'Class_70': 70, 'Class_71': 71, 'Class_72': 72, 'Class_73': 73, 'Class_74': 74, 'Class_75': 75, 'Class_76': 76, 'Class_77': 77, 'Class_78': 78, 'Class_79': 79, 'Class_80': 80, 'Class_81': 81, 'Class_82': 82, 'Class_83': 83, 'Class_84': 84, 'Class_85': 85, 'Class_86': 86, 'Class_87': 87, 'Class_88': 88, 'Class_89': 89, 'Class_90': 90, 'Class_91': 91, 'Class_92': 92, 'Class_93': 93, 'Class_94': 94, 'Class_95': 95, 'Class_96': 96, 'Class_97': 97, 'Class_98': 98, 'Class_99': 99, 'Class_100': 100, 'Class_101': 101, 'Class_102': 102, 'Class_103': 103, 'Class_104': 104, 'Class_105': 105, 'Class_106': 106, 'Class_107': 107, 'Class_108': 108, 'Class_109': 109, 'Class_110': 110, 'Class_111': 111, 'Class_112': 112, 'Class_113': 113, 'Class_114': 114, 'Class_115': 115, 'Class_116': 116, 'Class_117': 117, 'Class_118': 118, 'Class_119': 119, 'Class_120': 120, 'Class_121': 121, 'Class_122': 122, 'Class_123': 123, 'Class_124': 124, 'Class_125': 125, 'Class_126': 126, 'Class_127': 127, 'Class_128': 128, 'Class_129': 129, 'Class_130': 130, 'Class_131': 131, 'Class_132': 132, 'Class_133': 133, 'Class_134': 134, 'Class_135': 135, 'Class_136': 136, 'Class_137': 137, 'Class_138': 138, 'Class_139': 139, 'Class_140': 140, 'Class_141': 141, 'Class_142': 142, 'Class_143': 143, 'Class_144': 144, 'Class_145': 145, 'Class_146': 146, 'Class_147': 147, 'Class_148': 148, 'Class_149': 149, 'Class_150': 150, 'Class_151': 151, 'Class_152': 152, 'Class_153': 153, 'Class_154': 154, 'Class_155': 155, 'Class_156': 156, 'Class_157': 157, 'Class_158': 158, 'Class_159': 159, 'Class_160': 160, 'Class_161': 161, 'Class_162': 162, 'Class_163': 163, 'Class_164': 164, 'Class_165': 165, 'Class_166': 166, 'Class_167': 167, 'Class_168': 168, 'Class_169': 169, 'Class_170': 170, 'Class_171': 171, 'Class_172': 172, 'Class_173': 173, 'Class_174': 174, 'Class_175': 175, 'Class_176': 176, 'Class_177': 177, 'Class_178': 178, 'Class_179': 179, 'Class_180': 180, 'Class_181': 181, 'Class_182': 182, 'Class_183': 183, 'Class_184': 184, 'Class_185': 185, 'Class_186': 186, 'Class_187': 187, 'Class_188': 188, 'Class_189': 189, 'Class_190': 190, 'Class_191': 191, 'Class_192': 192, 'Class_193': 193, 'Class_194': 194, 'Class_195': 195, 'Class_196': 196, 'Class_197': 197, 'Class_198': 198, 'Class_199': 199, 'Class_200': 200, 'Class_201': 201, 'Class_202': 202, 'Class_203': 203, 'Class_204': 204, 'Class_205': 205, 'Class_206': 206, 'Class_207': 207, 'Class_208': 208, 'Class_209': 209, 'Class_210': 210, 'Class_211': 211, 'Class_212': 212, 'Class_213': 213, 'Class_214': 214, 'Class_215': 215, 'Class_216': 216, 'Class_217': 217, 'Class_218': 218, 'Class_219': 219, 'Class_220': 220, 'Class_221': 221, 'Class_222': 222, 'Class_223': 223, 'Class_224': 224, 'Class_225': 225, 'Class_226': 226, 'Class_227': 227, 'Class_228': 228, 'Class_229': 229, 'Class_230': 230, 'Class_231': 231, 'Class_232': 232, 'Class_233': 233, 'Class_234': 234, 'Class_235': 235, 'Class_236': 236, 'Class_237': 237, 'Class_238': 238, 'Class_239': 239, 'Class_240': 240, 'Class_241': 241, 'Class_242': 242, 'Class_243': 243, 'Class_244': 244, 'Class_245': 245, 'Class_246': 246, 'Class_247': 247, 'Class_248': 248, 'Class_249': 249, 'Class_250': 250, 'Class_251': 251, 'Class_252': 252, 'Class_253': 253, 'Class_254': 254, 'Class_255': 255, 'Class_256': 256, 'Class_257': 257, 'Class_258': 258, 'Class_259': 259, 'Class_260': 260, 'Class_261': 261, 'Class_262': 262, 'Class_263': 263, 'Class_264': 264, 'Class_265': 265, 'Class_266': 266, 'Class_267': 267, 'Class_268': 268, 'Class_269': 269, 'Class_270': 270, 'Class_271': 271, 'Class_272': 272, 'Class_273': 273, 'Class_274': 274, 'Class_275': 275, 'Class_276': 276, 'Class_277': 277, 'Class_278': 278, 'Class_279': 279, 'Class_280': 280, 'Class_281': 281, 'Class_282': 282, 'Class_283': 283, 'Class_284': 284, 'Class_285': 285, 'Class_286': 286, 'Class_287': 287, 'Class_288': 288, 'Class_289': 289, 'Class_290': 290, 'Class_291': 291, 'Class_292': 292, 'Class_293': 293, 'Class_294': 294, 'Class_295': 295, 'Class_296': 296, 'Class_297': 297, 'Class_298': 298, 'Class_299': 299, 'Class_300': 300, 'Class_301': 301, 'Class_302': 302, 'Class_303': 303, 'Class_304': 304, 'Class_305': 305, 'Class_306': 306, 'Class_307': 307, 'Class_308': 308, 'Class_309': 309, 'Class_310': 310, 'Class_311': 311, 'Class_312': 312, 'Class_313': 313, 'Class_314': 314, 'Class_315': 315, 'Class_316': 316, 'Class_317': 317, 'Class_318': 318, 'Class_319': 319, 'Class_320': 320, 'Class_321': 321, 'Class_322': 322, 'Class_323': 323, 'Class_324': 324, 'Class_325': 325, 'Class_326': 326, 'Class_327': 327, 'Class_328': 328, 'Class_329': 329, 'Class_330': 330, 'Class_331': 331, 'Class_332': 332, 'Class_333': 333, 'Class_334': 334, 'Class_335': 335, 'Class_336': 336, 'Class_337': 337, 'Class_338': 338, 'Class_339': 339, 'Class_340': 340, 'Class_341': 341, 'Class_342': 342, 'Class_343': 343, 'Class_344': 344, 'Class_345': 345, 'Class_346': 346, 'Class_347': 347, 'Class_348': 348, 'Class_349': 349, 'Class_350': 350}\n",
            "Missing Indices: {2, 3, 8, 21, 22, 24, 25, 30, 32, 35, 36, 37, 38, 39, 40, 47, 49, 51, 52, 55, 56, 59, 62, 63, 64, 65, 66, 67, 68, 69, 77, 82, 85, 88, 91, 92, 93, 94, 96, 97, 98, 101, 102, 103, 104, 106, 107, 109, 111, 112, 113, 115, 117, 119, 120, 121, 123, 126, 127, 128, 129, 130, 133, 135, 138, 140, 141, 142, 143, 146, 147, 148, 149, 151, 156, 157, 158, 159, 160, 162, 166, 168, 170, 171, 174, 176, 177, 179, 181, 183, 184, 186, 192, 193, 194, 195, 196, 197, 198, 200, 202, 204, 205, 207, 210, 217, 219, 220, 223, 224, 225, 228, 230, 232, 233, 234, 235, 238, 241, 243, 245, 247, 249, 250, 252, 253, 256, 259, 261, 262, 264, 265, 266, 267, 269, 270, 271, 272, 275, 276, 277, 278, 279, 280, 282, 283, 285, 287, 290, 292, 294, 297, 299, 302, 305, 308, 310, 311, 313, 316, 317, 319, 322, 323, 324, 325, 326, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 344, 346, 347, 348, 350}\n",
            "Warning: There are missing indices in the inverse_label_map!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 确定哪些索引没有在 inverse_label_map 中\n",
        "missing_indices = set(predictions.predictions.argmax(axis=-1).flatten().tolist()) - set(inverse_label_map.keys())\n",
        "\n",
        "# 如果有缺失的索引，可以手动添加它们到 inverse_label_map\n",
        "for idx in missing_indices:\n",
        "    if idx not in inverse_label_map:\n",
        "        inverse_label_map[idx] = f\"Class_{idx}\"\n",
        "\n",
        "print(\"Missing Indices Updated:\", missing_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqH-vx997nY7",
        "outputId": "29489d47-b346-43b6-93a5-f4f5881f0f79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Indices Updated: {2, 3, 8, 21, 22, 24, 25, 30, 32, 35, 36, 37, 38, 39, 40, 47, 49, 51, 52, 55, 56, 59, 62, 63, 64, 65, 66, 67, 68, 69, 77, 82, 85, 88, 91, 92, 93, 94, 96, 97, 98, 101, 102, 103, 104, 106, 107, 109, 111, 112, 113, 115, 117, 119, 120, 121, 123, 126, 127, 128, 129, 130, 133, 135, 138, 140, 141, 142, 143, 146, 147, 148, 149, 151, 156, 157, 158, 159, 160, 162, 166, 168, 170, 171, 174, 176, 177, 179, 181, 183, 184, 186, 192, 193, 194, 195, 196, 197, 198, 200, 202, 204, 205, 207, 210, 217, 219, 220, 223, 224, 225, 228, 230, 232, 233, 234, 235, 238, 241, 243, 245, 247, 249, 250, 252, 253, 256, 259, 261, 262, 264, 265, 266, 267, 269, 270, 271, 272, 275, 276, 277, 278, 279, 280, 282, 283, 285, 287, 290, 292, 294, 297, 299, 302, 305, 308, 310, 311, 313, 316, 317, 319, 322, 323, 324, 325, 326, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 344, 346, 347, 348, 350}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 确保 inverse_label_map 包含所有可能的标签索引\n",
        "inverse_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# 使用 DataLoader 加载测试数据集\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 遍历测试数据集\n",
        "for batch in test_loader:\n",
        "    # 将批次数据移动到模型所在的设备（例如：GPU）\n",
        "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "    # 模型评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 不计算梯度\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    # 获取预测结果的 logits 并找到概率最高的索引（预测的类别索引）\n",
        "    pred_labels_idx = outputs.logits.argmax(-1).tolist()  # 转换为 Python 列表\n",
        "\n",
        "    # 使用逆映射将索引转换为实际的标签\n",
        "    predicted_labels = [inverse_label_map.get(idx, 'Unknown Label') for idx in pred_labels_idx]\n",
        "\n",
        "    # 获取真实标签，这假设 'labels' 在批次数据中\n",
        "    actual_labels = [inverse_label_map.get(label.item(), 'Unknown Label') for label in batch['labels']]\n",
        "\n",
        "    # 打印预测结果和实际标签\n",
        "    for pred_label, actual_label in zip(predicted_labels, actual_labels):\n",
        "        print(f\"Predicted Label: {pred_label}, Actual Label: {actual_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ejyP1yHM6CFZ",
        "outputId": "b92f560e-5346-4d3a-9464-c6a73059e466"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n",
            "Predicted Label: Unknown Label, Actual Label: Unknown Label\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ffba0d9d8966>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 不计算梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# 获取预测结果的 logits 并找到概率最高的索引（预测的类别索引）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1692\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 )\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0;31m# In this case we skip the first parameter of the underlying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0;31m# function (usually `self` or `cls`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_signature_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2461\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2464\u001b[0m                                         \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_POSITIONAL_ONLY\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_POSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[0m\u001b[1;32m   2326\u001b[0m                                     kind=kind))\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2639\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ParameterKind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2640\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'value {kind!r} is not a valid Parameter.kind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[1;32m    361\u001b[0m         \u001b[0mEither\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0mexisting\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhw1c9sA7IQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}